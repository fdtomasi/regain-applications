{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Models for Textual Data\n",
    "This shows how graphical models can be used to infer relationships between textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from regain.utils import flatten\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from regain.data import base; import imp; imp.reload(base)\n",
    "train, test = base.load_webkb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - create data manually (no filtering on terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the dictionary of the words (only strings)\n",
    "words = np.unique(flatten([words.split(' ') for words in train.words.tolist() if isinstance(words, str)]))\n",
    "\n",
    "# for each document, create the TermFrequency\n",
    "tf_docs = [dict(Counter(v.split(' '))) for k, v in train.itertuples() if isinstance(v, str)]\n",
    "assert len(tf_docs) == len(train)\n",
    "\n",
    "# build data\n",
    "X = pd.DataFrame(tf_docs, index=[row.Index for row in train.itertuples()\n",
    "      if isinstance(row.words, str)]).fillna(0)\n",
    "y = X.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - create data with some filters based on sklearn-`CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=1., min_df=0.) #, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(train.values.flatten())\n",
    "\n",
    "df_tf = pd.DataFrame(tf.todense(), index=train.index, columns=tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X, df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NMF is able to use tf-idf\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# # Run NMF\n",
    "# nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# nmf_W = nmf_model.transform(tfidf)\n",
    "# nmf_H = nmf_model.components_\n",
    "\n",
    "# print(\"NMF Topics\")\n",
    "# display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online',\n",
    "                                      learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain import utils_text; imp.reload(utils_text)\n",
    "\n",
    "print(\"LDA Topics\")\n",
    "topics = utils_text.display_topics(lda_H, lda_W, tf_vectorizer.get_feature_names(),\n",
    "                                   train.values.flatten(), n_top_words=4, n_top_documents=1, print_docs=False)\n",
    "\n",
    "df = pd.DataFrame(lda_W, index=train.index, columns=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogEntropyModel (`regain`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_discard = []\n",
    "for yy in np.unique(y):\n",
    "    words_to_discard += list(X[words[X[y==yy].sum(axis=0) == 0]].columns)\n",
    "\n",
    "words_to_keep = list(set(X.columns) - set(words_to_discard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federicot/src/fdtomasi/regain/regain/utils_text.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  E = 1 + (P * np.log(P)).fillna(0).values.sum(\n"
     ]
    }
   ],
   "source": [
    "X_new = utils_text.logentropy_normalize(X)[words_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogEntropyModel (`gensim`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LogEntropyModel, LdaModel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "corp = [w.split(' ') for w in train.words if isinstance(w, str)]\n",
    "text = corp #common_texts\n",
    "dct = Dictionary(text)  # fit dictionary\n",
    "\n",
    "num_terms = 50 # or words.size\n",
    "dct.filter_extremes(keep_n=num_terms)\n",
    "\n",
    "corpus = [dct.doc2bow(row) for row in text]  #convert to BoW format\n",
    "model = LogEntropyModel(corpus, normalize=True)  # fit model\n",
    "\n",
    "# model = models.LdaModel(corpus, id2word=dct, num_topics=num_terms)\n",
    "\n",
    "import gensim\n",
    "X_new = gensim.matutils.corpus2dense(model[corpus], num_terms=num_terms).T\n",
    "df_new = pd.DataFrame(X_new, columns=list(dct.values()), index=y)\n",
    "\n",
    "# df[words_to_keep].T.sort_index().T\n",
    "# X = df[words_to_keep].values\n",
    "\n",
    "# X = df.values\n",
    "# y = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from regain.covariance import kernel_time_graphical_lasso_\n",
    "from regain.model_selection import stability_optimization\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mdl = kernel_time_graphical_lasso_.KernelTimeGraphicalLasso(\n",
    "    verbose=0, kernel=np.ones((np.unique(y).size, np.unique(y).size)), psi='l1',\n",
    "    alpha=0.45, max_iter=1000).fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(stability_optimization);\n",
    "socv = stability_optimization.GraphicalModelStabilitySelection(\n",
    "    mdl, param_grid=dict(alpha=np.logspace(2, -2)),\n",
    "    sampling_size=200,\n",
    ").fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times = np.unique(y).size\n",
    "n_dim = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.triu_indices(n_dim, 1)\n",
    "dof = idx[0].size * n_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = socv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero percentage: 0.7133\n"
     ]
    }
   ],
   "source": [
    "print(\"Nonzero percentage: %.4f\" % (np.sum([np.count_nonzero(P[idx]) for P in mdl.precision_]) / dof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from kdge import plot_plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "py.init_notebook_mode()\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mdl.precision_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import retain_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = -1\n",
    "trace = []\n",
    "graphs = []\n",
    "# for i, p in enumerate(ltgl.precision_ - ltgl.latent_):\n",
    "for i, p in enumerate(mdl.precision_):\n",
    "\n",
    "    A = np.abs(p - np.diag(np.diag(p)))\n",
    "    A = retain_top_n(A, top_n)\n",
    "    G = nx.from_numpy_matrix(A * 0.00001)\n",
    "    graphs.append(G)\n",
    "    trace.append(pl.plot_circular(G, df.columns, 1.4, cmap='Blues',\n",
    "                                  #color_nodes=plt.rcParams['axes.prop_cycle'].by_key()['color'][:5]\n",
    "                                 ))\n",
    "\n",
    "fig = tools.make_subplots(\n",
    "    rows=1, cols=len(mdl.precision_), horizontal_spacing=.1, print_grid=False)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    tr['data'][k]['xaxis'] = 'x' + str(j+1)\n",
    "    tr['data'][k]['yaxis'] = 'y' + str(j+1)\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    for i, x in enumerate(tr['data']):\n",
    "        col = j + 1\n",
    "        x['legendgroup'] = 'group'+ str(j+1)\n",
    "        x['showlegend'] = False\n",
    "        fig.append_trace(x,1,col)\n",
    "        \n",
    "\n",
    "\n",
    "for j, tr in enumerate(trace):\n",
    "    fig.layout.annotations += tuple([pl._set_ref(\n",
    "        x, 'x'+ str(j+1), 'y'+ str(j+1)) for x in tr['layout']['annotations']])\n",
    "\n",
    "    fig['layout']['xaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    fig['layout']['yaxis'+str(j+1)].update(showgrid=False, zeroline=False, showticklabels=False)\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "fig['layout'].update(height=900, width=4000,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_image(fig, \"graphs.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctmmodel\n",
    "ctm_model = ctmmodel.CtmModel(corpus, id2word=dct, num_topics=15)\n",
    "\n",
    "all_words = []\n",
    "for c in corpus:\n",
    "    doc_words = []\n",
    "    for cc in c:\n",
    "        doc_words.extend([dct[cc[0]]] * cc[1])\n",
    "    all_words.append(' '.join(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyctm\n",
    "from pyctm import variational_bayes, inferencer, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter set 3\n",
    "alpha_mu=0.\n",
    "alpha_sigma=1\n",
    "alpha_beta=0\n",
    "\n",
    "ctm_inferencer = variational_bayes.VariationalBayes();\n",
    "ctm_inferencer._initialize(\n",
    "    all_words, list(dct.values()),\n",
    "    number_of_topics=15,\n",
    "    alpha_mu=alpha_mu, alpha_sigma=alpha_sigma,\n",
    "    alpha_beta=alpha_beta);\n",
    "# ctm_inferencer._initialize(train.words.tolist(), words, 20,\n",
    "#                            alpha_mu, alpha_sigma, alpha_beta);\n",
    "\n",
    "for iteration in range(50):\n",
    "    ctm_inferencer.learning(-1)\n",
    "\n",
    "logl, lamda, nu = ctm_inferencer.inference(all_words)\n",
    "# logl, lamda, nu = ctm_inferencer.inference(train.words.tolist())\n",
    "\n",
    "ll = utils.topic_beta(ctm_inferencer)\n",
    "\n",
    "topic_words = pd.DataFrame(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_str_repr = []\n",
    "for row in topic_words.iterrows():\n",
    "    print(pd.DataFrame(row[1].sort_values(ascending=False)[:3]).T)\n",
    "    topic_str_repr.append(' '.join(row[1].sort_values(ascending=False)[:3].index))\n",
    "\n",
    "word_dct_values = list(dct.values())\n",
    "\n",
    "dff = pd.DataFrame(ctm_model.beta, columns=word_dct_values)\n",
    "dff = dff[sorted(dff.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "\n",
    "from regain import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gl = GraphicalLassoCV().fit(lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gl.precision_\n",
    "\n",
    "A = np.abs(p - np.diag(np.diag(p)))\n",
    "A = retain_top_n(A, 20)\n",
    "G = nx.from_numpy_matrix(A * 3)\n",
    "fig = pl.plot_circular(G, topic_str_repr, 2, cmap='Blues')\n",
    "\n",
    "fig['layout'].update(height=800, width=800,hovermode='closest',\n",
    "                     paper_bgcolor='rgba(0,0,0,0)',\n",
    "                        plot_bgcolor='rgba(0,0,0,0)')\n",
    "# fig.layout.annotations += tuple([dict(\n",
    "#     text=\"Python code: <a href='https://plot.ly/ipython-notebooks/network-graphs/'> https://plot.ly/ipython-notebooks/network-graphs/</a>\",\n",
    "#     showarrow=False, xref=\"paper\", yref=\"paper\", x=0.005, y=-0.2)])\n",
    "# fig['layout'].update(scene=dict(aspectmode=\"data\"))\n",
    "py.iplot(fig)\n",
    "# py.offline.iplot(fig, filename='figure_factory_subplot', image='svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
